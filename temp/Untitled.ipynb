{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d331d8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch import nn, LongTensor, Tensor\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e42544a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tin/.cache/huggingface/hub\n"
     ]
    }
   ],
   "source": [
    "from transformers.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
    "print(str(PYTORCH_PRETRAINED_BERT_CACHE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a873a824",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_labels = [\"rel:None\", \"rel:True\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49d79965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "bert = BertModel.from_pretrained(model_name, cache_dir=str(PYTORCH_PRETRAINED_BERT_CACHE))\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name, cache_dir=str(PYTORCH_PRETRAINED_BERT_CACHE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1aadae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(re_labels)\n",
    "\n",
    "dropout = nn.Dropout(bert.config.hidden_dropout_prob)\n",
    "layer_norm = nn.LayerNorm(bert.config.hidden_size * 2)\n",
    "classifer = nn.Linear(bert.config.hidden_size * 2, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72d2224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/data.pkl', 'rb') as f:\n",
    "    features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20352a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45413\n",
      "31791\n",
      "6811\n",
      "6811\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(features)\n",
    "\n",
    "features_len = len(features)\n",
    "print(features_len)\n",
    "start = 0\n",
    "end = start + int(features_len * 0.15)\n",
    "val_features = features[start:end]\n",
    "\n",
    "start = end\n",
    "end = start + int(features_len * 0.15)\n",
    "test_features = features[start:end]\n",
    "\n",
    "start = end\n",
    "train_features = features[start:]\n",
    "print(len(train_features))\n",
    "print(len(val_features))\n",
    "print(len(test_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4151e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_a_dataset(features, batch_size):\n",
    "\n",
    "    all_input_ids = pad_sequence([LongTensor(f.input_ids) for f in features],\n",
    "                                batch_first=True)\n",
    "    all_input_mask = pad_sequence([LongTensor(f.input_mask) for f in features],\n",
    "                                batch_first=True)\n",
    "    all_segment_ids = pad_sequence([LongTensor(f.segment_ids) for f in features],\n",
    "                                batch_first=True)\n",
    "    all_label_id = LongTensor([f.label_id for f in features]).unsqueeze(1)\n",
    "    all_sub_idx = pad_sequence([LongTensor([f.sub_idx]) for f in features],\n",
    "                                batch_first=True)\n",
    "    all_obj_idx = pad_sequence([LongTensor([f.obj_idx]) for f in features],\n",
    "                                batch_first=True)\n",
    "    data = TensorDataset(all_input_ids, all_input_mask,\n",
    "                        all_segment_ids, all_label_id, all_sub_idx,\n",
    "                        all_obj_idx)\n",
    "    dataloader = DataLoader(data,\n",
    "                            batch_size=batch_size)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5435dbad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "994"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = _create_a_dataset(train_features, 32)\n",
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81056a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 170])\n",
      "torch.Size([32, 170])\n",
      "torch.Size([32, 170])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "train_batches = [batch for batch in train_loader]\n",
    "for item in train_batches[0]:\n",
    "    print(item.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6c3784f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[  101,   100,   100,  ...,     0,     0,     0],\n",
       "         [  101,   100,   100,  ...,     0,     0,     0],\n",
       "         [  101,   100,  2744,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,   100,   100,  ...,     0,     0,     0],\n",
       "         [  101,   100,   100,  ...,     0,     0,     0],\n",
       "         [  101, 30522,   100,  ...,     0,     0,     0]]),\n",
       " tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " tensor([[0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]]),\n",
       " tensor([[18],\n",
       "         [14],\n",
       "         [14],\n",
       "         [ 3],\n",
       "         [ 3],\n",
       "         [28],\n",
       "         [ 1],\n",
       "         [29],\n",
       "         [ 5],\n",
       "         [ 7],\n",
       "         [28],\n",
       "         [19],\n",
       "         [ 1],\n",
       "         [18],\n",
       "         [ 1],\n",
       "         [52],\n",
       "         [ 4],\n",
       "         [ 3],\n",
       "         [ 1],\n",
       "         [28],\n",
       "         [18],\n",
       "         [40],\n",
       "         [39],\n",
       "         [ 1],\n",
       "         [ 1],\n",
       "         [18],\n",
       "         [25],\n",
       "         [ 2],\n",
       "         [18],\n",
       "         [20],\n",
       "         [22],\n",
       "         [ 1]]),\n",
       " tensor([[54],\n",
       "         [61],\n",
       "         [22],\n",
       "         [30],\n",
       "         [37],\n",
       "         [68],\n",
       "         [21],\n",
       "         [45],\n",
       "         [ 9],\n",
       "         [13],\n",
       "         [57],\n",
       "         [48],\n",
       "         [26],\n",
       "         [72],\n",
       "         [12],\n",
       "         [68],\n",
       "         [51],\n",
       "         [23],\n",
       "         [40],\n",
       "         [36],\n",
       "         [43],\n",
       "         [48],\n",
       "         [49],\n",
       "         [24],\n",
       "         [45],\n",
       "         [74],\n",
       "         [56],\n",
       "         [27],\n",
       "         [25],\n",
       "         [63],\n",
       "         [37],\n",
       "         [32]])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c580264b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30538, 768)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(tokenizer) = pre calculated\n",
    "tokenizer_len = 30538\n",
    "bert.resize_token_embeddings(tokenizer_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ed51d1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, input_mask, segment_ids, label_ids, sub_idx, obj_idx = train_batches[0]\n",
    "\n",
    "# loss = model(input_ids, segment_ids, input_mask, label_ids,\n",
    "#              sub_idx, obj_idx)\n",
    "\n",
    "outputs = bert(input_ids,\n",
    "                    token_type_ids=segment_ids,\n",
    "                    attention_mask=input_mask,\n",
    "                    output_hidden_states=False,\n",
    "                    output_attentions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "260f286b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 170, 768])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_output = outputs[0]\n",
    "sequence_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9dc55334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 768]), torch.Size([32, 768]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_output = torch.cat(\n",
    "            [a[i].unsqueeze(0) for a, i in zip(sequence_output, sub_idx)]).squeeze(1)\n",
    "obj_output = torch.cat(\n",
    "            [a[i].unsqueeze(0) for a, i in zip(sequence_output, obj_idx)]).squeeze(1)\n",
    "sub_output.shape, obj_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2971bfa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1536])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rep = torch.cat((sub_output, obj_output), dim=1)\n",
    "rep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1be67838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1536])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rep = layer_norm(rep)\n",
    "rep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "df2dece2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1536])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rep = dropout(rep)\n",
    "rep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "120fd3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = classifer(rep)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f4e29217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4979, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fct = CrossEntropyLoss()\n",
    "loss = loss_fct(logits.view(-1, 2), label_ids.view(-1))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8963411c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_ids.view(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ae7c1dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4741, -0.7999],\n",
       "        [-0.3342, -0.4986],\n",
       "        [-0.8379, -0.3600],\n",
       "        [ 0.0315, -0.2533],\n",
       "        [-0.3554, -0.9357],\n",
       "        [ 0.2258, -0.6719],\n",
       "        [ 0.6394, -0.6025],\n",
       "        [ 0.1216, -0.9999],\n",
       "        [ 0.1489, -1.7862],\n",
       "        [ 0.1915, -1.3544],\n",
       "        [ 0.0734, -0.6052],\n",
       "        [ 0.5974, -0.6603],\n",
       "        [ 0.2991, -0.9130],\n",
       "        [ 0.0100, -0.6190],\n",
       "        [-0.3008, -0.8990],\n",
       "        [-0.2774, -0.9363],\n",
       "        [-0.2936, -0.5965],\n",
       "        [ 0.3739, -1.0638],\n",
       "        [-0.6579, -0.6168],\n",
       "        [-0.2230, -0.7594],\n",
       "        [-0.5448, -0.9943],\n",
       "        [ 0.1981, -0.2109],\n",
       "        [-0.1782, -1.2148],\n",
       "        [ 0.5937, -0.5384],\n",
       "        [-0.3755, -0.7256],\n",
       "        [-0.2742, -0.8289],\n",
       "        [-0.1143, -0.2231],\n",
       "        [-0.2555, -0.9518],\n",
       "        [-0.7545, -0.8190],\n",
       "        [ 0.0631, -0.5730],\n",
       "        [ 0.2937, -0.6628],\n",
       "        [ 0.2568, -0.6015]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.view(-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "df7f9cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_ids.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e1429b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7814, 0.2186],\n",
       "        [0.5410, 0.4590],\n",
       "        [0.3827, 0.6173],\n",
       "        [0.5707, 0.4293],\n",
       "        [0.6411, 0.3589],\n",
       "        [0.7105, 0.2895],\n",
       "        [0.7759, 0.2241],\n",
       "        [0.7543, 0.2457],\n",
       "        [0.8738, 0.1262],\n",
       "        [0.8243, 0.1757],\n",
       "        [0.6634, 0.3366],\n",
       "        [0.7786, 0.2214],\n",
       "        [0.7707, 0.2293],\n",
       "        [0.6523, 0.3477],\n",
       "        [0.6452, 0.3548],\n",
       "        [0.6590, 0.3410],\n",
       "        [0.5751, 0.4249],\n",
       "        [0.8081, 0.1919],\n",
       "        [0.4897, 0.5103],\n",
       "        [0.6310, 0.3690],\n",
       "        [0.6105, 0.3895],\n",
       "        [0.6009, 0.3991],\n",
       "        [0.7382, 0.2618],\n",
       "        [0.7562, 0.2438],\n",
       "        [0.5866, 0.4134],\n",
       "        [0.6352, 0.3648],\n",
       "        [0.5272, 0.4728],\n",
       "        [0.6674, 0.3326],\n",
       "        [0.5161, 0.4839],\n",
       "        [0.6539, 0.3461],\n",
       "        [0.7224, 0.2776],\n",
       "        [0.7023, 0.2977]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fcd3d225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8547, 0.1453],\n",
       "        [0.4956, 0.5044],\n",
       "        [0.5554, 0.4446],\n",
       "        [0.5967, 0.4033],\n",
       "        [0.6468, 0.3532],\n",
       "        [0.6322, 0.3678],\n",
       "        [0.8572, 0.1428],\n",
       "        [0.6937, 0.3063],\n",
       "        [0.8479, 0.1521],\n",
       "        [0.6960, 0.3040],\n",
       "        [0.6972, 0.3028],\n",
       "        [0.6432, 0.3568],\n",
       "        [0.7775, 0.2225],\n",
       "        [0.7266, 0.2734],\n",
       "        [0.7077, 0.2923],\n",
       "        [0.3962, 0.6038],\n",
       "        [0.6270, 0.3730],\n",
       "        [0.7101, 0.2899],\n",
       "        [0.5694, 0.4306],\n",
       "        [0.5693, 0.4307],\n",
       "        [0.5657, 0.4343],\n",
       "        [0.6056, 0.3944],\n",
       "        [0.7152, 0.2848],\n",
       "        [0.6122, 0.3878],\n",
       "        [0.5844, 0.4156],\n",
       "        [0.6143, 0.3857],\n",
       "        [0.5862, 0.4138],\n",
       "        [0.7572, 0.2428],\n",
       "        [0.4904, 0.5096],\n",
       "        [0.6593, 0.3407],\n",
       "        [0.6805, 0.3195],\n",
       "        [0.6168, 0.3832]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fcbb349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data and create dataloader for models\n",
    "# For binary relation extraction\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch import LongTensor\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
    "\n",
    "\n",
    "class TempRelationProcessor:\n",
    "\n",
    "    def __init__(self, data: list[dict], model_name: str = 'bert-base-uncased', batch_size: int = 128):\n",
    "        self.data = data\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def run(self):\n",
    "        tokenizer = BertTokenizer.from_pretrained(self.model_name, cache_dir=str(PYTORCH_PRETRAINED_BERT_CACHE))\n",
    "        ner_labels = ['concept', 'writing', 'person', 'place']\n",
    "\n",
    "        tokenizer = self._add_marker_tokens(tokenizer, ner_labels)\n",
    "\n",
    "        features = self._add_typed_markers(self.data, tokenizer)\n",
    "        print(len(tokenizer))\n",
    "\n",
    "        \n",
    "    def _add_marker_tokens(self, tokenizer, ner_labels):\n",
    "        new_tokens = []\n",
    "\n",
    "        for label in ner_labels:\n",
    "            new_tokens.append('<SUBJ_START=%s>' % label)\n",
    "            new_tokens.append('<SUBJ_END=%s>' % label)\n",
    "            new_tokens.append('<OBJ_START=%s>' % label)\n",
    "            new_tokens.append('<OBJ_END=%s>' % label)\n",
    "\n",
    "        tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "        return tokenizer\n",
    "\n",
    "\n",
    "    def _add_typed_markers(self, data, tokenizer):\n",
    "        \"\"\"\n",
    "        Inspired by PURE\n",
    "        \"\"\"\n",
    "        \n",
    "        CLS = \"[CLS]\"\n",
    "        SEP = \"[SEP]\"\n",
    "\n",
    "        def get_special_token(w):\n",
    "            return ('<' + w + '>').lower()\n",
    "        \n",
    "        max_tokens = 0\n",
    "        total_tokens = 0\n",
    "        features = []\n",
    "        for (sent_idx, sentence) in enumerate(data):\n",
    "            if sent_idx % 10000 == 0:\n",
    "                print(f\"Adding typed markers: {sent_idx} of {len(data)}\")\n",
    "\n",
    "            subj_entity = sentence['entities'][0]\n",
    "            obj_entity  = sentence['entities'][1]\n",
    "\n",
    "            # Create typed markers\n",
    "            SUBJECT_START_MARKER = get_special_token(f\"SUBJ_START={subj_entity['type']}\")\n",
    "            SUBJECT_END_MARKER  = get_special_token(f\"SUBJ_END={subj_entity['type']}\")\n",
    "            OBJECT_START_MARKER  = get_special_token(f\"OBJ_START={obj_entity['type']}\")\n",
    "            OBJECT_END_MARKER  = get_special_token(f\"OBJ_END={obj_entity['type']}\")\n",
    "\n",
    "            # Create marked sentence\n",
    "            sub_idx = 0\n",
    "            obj_idx = 0\n",
    "            marked_sentence = []\n",
    "            marked_sentence.append(CLS)\n",
    "            for i, token in enumerate(sentence['sentence']):\n",
    "                if i == subj_entity['start_pos']:\n",
    "                    sub_idx = len(marked_sentence)\n",
    "                    marked_sentence.append(SUBJECT_START_MARKER)\n",
    "                if i == obj_entity['start_pos']:\n",
    "                    obj_idx = len(marked_sentence)\n",
    "                    marked_sentence.append(OBJECT_START_MARKER)\n",
    "\n",
    "                for sub_token in tokenizer.tokenize(token):\n",
    "                    marked_sentence.append(token)\n",
    "\n",
    "                if i == subj_entity['end_pos']:\n",
    "                    marked_sentence.append(SUBJECT_END_MARKER)\n",
    "                if i == obj_entity['end_pos']:\n",
    "                    marked_sentence.append(OBJECT_END_MARKER)\n",
    "            marked_sentence.append(SEP)\n",
    "\n",
    "            max_tokens = max(max_tokens, len(marked_sentence))\n",
    "            total_tokens += len(marked_sentence)\n",
    "\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(marked_sentence)\n",
    "            input_mask = [1] * len(input_ids)\n",
    "            segment_ids = [0] * len(marked_sentence)\n",
    "            label_id = sentence['relation']\n",
    "\n",
    "            features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                            input_mask=input_mask,\n",
    "                            segment_ids=segment_ids,\n",
    "                            label_id=label_id,\n",
    "                            sub_idx=sub_idx,\n",
    "                            obj_idx=obj_idx))\n",
    "\n",
    "        print(\"Adding typed markers: Done\")\n",
    "        print(f\"Total tokens: {total_tokens}\")\n",
    "        print(f\"Max tokens  : {max_tokens}\")\n",
    "\n",
    "\n",
    "        return features\n",
    "     \n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id, sub_idx,\n",
    "                 obj_idx):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        self.sub_idx = sub_idx\n",
    "        self.obj_idx = obj_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61e545ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = [json.loads(line) for line in open('../data/all_data_transformed.json', 'r')]\n",
    "\n",
    "processor = TempRelationProcessor(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "310a7f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding typed markers: 0 of 45413\n",
      "Adding typed markers: 10000 of 45413\n",
      "Adding typed markers: 20000 of 45413\n",
      "Adding typed markers: 30000 of 45413\n",
      "Adding typed markers: 40000 of 45413\n",
      "Adding typed markers: Done\n",
      "Total tokens: 3107446\n",
      "Max tokens  : 170\n",
      "30538\n"
     ]
    }
   ],
   "source": [
    "processor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16b2ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "temp = BertTokenizer.from_pretrained(model_name, cache_dir=str(PYTORCH_PRETRAINED_BERT_CACHE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a008dbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "bert = BertModel.from_pretrained(model_name, cache_dir=str(PYTORCH_PRETRAINED_BERT_CACHE))\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name, cache_dir=str(PYTORCH_PRETRAINED_BERT_CACHE))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
